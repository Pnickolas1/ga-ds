{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Part 2\n",
    "\n",
    "\n",
    "## Topics:\n",
    "* Recap\n",
    "* Statsmodels\n",
    "* Adjusted R^2\n",
    "\n",
    "\n",
    "* Categorical Variables\n",
    "    * Why k-1 variables?\n",
    "  \n",
    "* Interaction Terms\n",
    "* Polynomial Variables\n",
    "\n",
    "* What about Many levels per categorical variable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Recall that linear regression attempts to fit a response variable $Y$ to a linear combination of variables $X$, and their associated coefficients, $\\beta$.\n",
    "\n",
    "While we know the data points for $Y$ and $X$, we must estimate $\\beta$. The OLS (Ordinary Least Squares) estimate for $\\beta$ is:\n",
    "\n",
    "\n",
    "$\\hat{\\beta} = {(X'X)}^{-1}X'y$\n",
    "\n",
    "Once we have estimates for $\\beta$, we can perform:\n",
    "* Prediction (Predictions are on data points encoded as $X$)\n",
    "* Weight Analysis (How much does each factor contribute?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from patsy import dmatrices\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"http://data.princeton.edu/wws509/datasets/salary.dat\", sep='\\s+')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, lets try using the sklearn linear model\n",
    "X = data[ ['yr','yd'] ]\n",
    "y = data[ 'sl' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object. We'll explicitly include an intercept term\n",
    "model = linear_model.LinearRegression(fit_intercept=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the beta coefficients using X and y\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "print model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get R^2 score once you have fitted the model\n",
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can also calculate the R^2 manually\n",
    "# R^2 = 1 - SSE/SST\n",
    "#   where SSE is the square of the sum of errors\n",
    "#   and SST is the square of the sum of deviations from the mean\n",
    "sse = np.power(y_pred - y,2).sum()\n",
    "sst = np.power(y - y.mean(),2).sum()\n",
    "score = 1.0 - (sse/sst)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model form\n",
    "print \"sl ~ \", str(model.intercept_),\"+ \",model.coef_[0],\"* yr +\",model.coef_[1],\"* yd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single train/test split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n",
    "\n",
    "model = linear_model.LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train,y_train)\n",
    "print \"Train Score:\", model.score(X_train,y_train)\n",
    "print \"Test Score:\", model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running once gives us a lot of variance in train/test, let's try cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "model = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Instead of cv=5, or cv=10, I use KFold which is an explicit iterator for the data.\n",
    "# Normally cross_val_score does not shuffle by default. KFold allows shuffling of data\n",
    "cv_scores = cross_val_score(model,X,y,cv=KFold(len(X),n_folds=10,shuffle=True))\n",
    "\n",
    "print \"Avg CV Score:\", cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels\n",
    "\n",
    "- Statsmodels is a relatively new package that provides convenient utilities for investigating the results of a model. \n",
    "- It uses `patsy` to provide R formula syntax\n",
    "\n",
    "A formula allows you to write a functional relationship between variables.  \n",
    "Example:\n",
    "\n",
    "```R\n",
    "Y ~ X1 + X2 + X3\n",
    "```\n",
    "\n",
    "The formulas automatically includes an intercept term. You can make this explicit by using \n",
    "\n",
    "```R\n",
    "Y ~ 1 + X1 + X2 + X3\n",
    "```\n",
    "\n",
    "http://statsmodels.sourceforge.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Formula as above. Fit salary on an intercept, yr, and yd\n",
    "model = sm.ols(formula=\"sl ~ yr + yd\", data=data).fit() # Call fit on the ols model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "Using statsmodels, we can encode the 'formula' directly into the model, instead of manually creating design matrices through patsy. In fact, statsmodels uses patsy under the hood to make the process easy.\n",
    "\n",
    "* [Patsy](https://patsy.readthedocs.org/en/latest/)\n",
    "* [Statsmodels](http://statsmodels.sourceforge.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can add categorical variables easily!\n",
    "model = sm.ols(formula=\"sl ~ yr + yd + rk\", data=data).fit() # Call fit on the ols model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Question: Why doesn't patsy/statsmodels include rk[T.assistant] as well?**\n",
    "\n",
    "If we add **rk[T.assisstant]** to the model, this creates a rank deficient matrix, and causes a singularity when computing ${(X'X)}^{-1}$. This is called the [Dummy Variable Trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html).\n",
    "\n",
    "In short, if you did add rk[T.assisstant] as a factor to your matrix, the column sums of rk[T.full] + rk[T.associate] + rk[T.assistant] = 1 for every row. The column sums == the intercept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** Question: Then, how do I interpret the above coefficients? **\n",
    "\n",
    "You can look it this way. When rk[T.associate]==0 and rk[T.full]==0, then resulting salary from the linear regression is when the rank is assistant. How? The **intercept**!\n",
    "\n",
    "```\n",
    "sl ~ Intercept + Rk[Associate] + Rk[Full] + Yr + Yd\n",
    "```\n",
    "\n",
    "If either rk[T.associate] == 1 OR rk[T.full] == 1, you can look at their respective beta coefficients as the marginal contribution to overall salary. The intercept encodes the case when rank == Assistant\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Factors\n",
    "\n",
    "```\n",
    "sm.ols(formula=\"sl ~ yr + yd + rk\", data=data)\n",
    "```\n",
    "\n",
    "As you can see in the above examplaes, `+` is not acting as an addition operator but as a separator between other variables.\n",
    "\n",
    "There are other operators that lose their algebraic meaning in a formula. \n",
    "\n",
    "- `+` adds a new variable.\n",
    "- `:` adds the _interaction_ of two variables. \n",
    "- `*` adds the original terms as well as their interaction effect.\n",
    "- `C(yr)` adds a set of terms where yr is converted to dummy variables\n",
    "- `np.power(x,2)` adds a term where x is raised to the second power\n",
    "\n",
    "[Formula types](https://patsy.readthedocs.org/en/latest/formulas.html#the-formula-language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try one of these interaction terms\n",
    "# Adds yr, rk, yr*rk (and their combinations)\n",
    "model = sm.ols(formula=\"sl ~ yr*rk\", data=data).fit()\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "How do I know whether the current model I have is bad or not?\n",
    "\n",
    "0. $R^2$/ Adjusted $R^2$\n",
    "1. Correlation matrix\n",
    "2. T Stat/P Values\n",
    "3. F statistic\n",
    "4. Skew/Kurtosis/JB Test\n",
    "5. In-sample/out-of-sample predictions\n",
    "\n",
    "\n",
    "- Adjusted $R^2$ takes into account the number of additional factors you have and penalizes for that (i.e. you can't just keep adding factors until you get a perfect solution)\n",
    "- Correlation matrix of the factors helps you apriori find collinear factors\n",
    "- T-stat/P-values of beta coefficients help diagnose when factors are not contributing\n",
    "- Skew/Kurtosis/JB Test are somewhat second-order stats and help diagnose whether the errors are truly normally distributed\n",
    "- In-Sample/OOS predictions are for the most part the way to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's go over the analysis terms\n",
    "model = sm.ols(formula=\"sl ~ yr*rk\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test 1 -- Correlations of Factors, going to use Patsy here for automatic column names\n",
    "y,X = dmatrices(\"sl ~ yr*rk\",data=data,return_type=\"dataframe\")\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **yr:rk[T.full]** factor is pretty correlated with **rk[T.full]** as well as **yr**. \n",
    "\n",
    "This makes sense as the interaction variable is just yr * \"Is rank full\". \n",
    "\n",
    "** Question: What can happen when there are collinear(similar) variables in a model?**\n",
    "** Answer: It depends **\n",
    "* If the level of collinearity is low, the predictions may not change too much, but the beta values will not make too much sense--hard to get an intuition\n",
    "* If the degree of collinearity is high however, the predictions may change drastically based on small changes in the input. Chalkboard example to follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Polynomial Terms\n",
    "\n",
    "We can also add polynomial (i.e. X^2, X^4, etc.) terms as factors. If there is some nonlinearity that we want to capture, but still use a linear model, we can add terms as polynomials.\n",
    "\n",
    "Let's take a look at how much the model predictions change based on how many polynomial terms we add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data points we're going to use first\n",
    "data = pd.DataFrame({'x': np.linspace(0,1,8),\n",
    "                     'y': [1.0,1.4,1.6,1.4,1.7,1.45,1.7,2.0] })\n",
    "plt.scatter(data.x,data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a model of y ~ Intercept + x\n",
    "model = sm.ols(\"y ~ x\",data=data).fit()\n",
    "model.summary()\n",
    "x_new = np.linspace(0,1,100)\n",
    "y_new = model.params[0] + model.params[1] * x_new\n",
    "plt.scatter(data.x,data.y)\n",
    "plt.plot(x_new,y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a model of y ~ Intercept + x + x^2\n",
    "model = sm.ols(\"y ~ x + np.power(x,2) \",data=data).fit()\n",
    "x_new = np.linspace(0,1,100)\n",
    "y_new = model.params[0] + model.params[1] * x_new + model.params[2] * np.power(x_new,2)\n",
    "plt.scatter(data.x,data.y)\n",
    "plt.plot(x_new,y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a model of y ~ Intercept + x + x^2 + x^3\n",
    "model = sm.ols(\"y ~ x + np.power(x,2) + np.power(x,3)\",data=data).fit()\n",
    "x_new = np.linspace(0,1,100)\n",
    "\n",
    "y_new = model.params[0] + \\\n",
    "    model.params[1] * x_new + \\\n",
    "    model.params[2] * np.power(x_new,2) + \\\n",
    "    model.params[3] * np.power(x_new,3)\n",
    "\n",
    "plt.scatter(data.x,data.y)\n",
    "plt.plot(x_new,y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a model of y ~ Intercept + x + x^2 + x^3 + x^4 + x^5 + x^6 + x^7 + ...\n",
    "model = sm.ols(\"y ~ x + np.power(x,2) + np.power(x,3) + np.power(x,4) + np.power(x,5) + np.power(x,6) + np.power(x,7) + np.power(x,8) + np.power(x,9)\",data=data).fit()\n",
    "x_new = np.linspace(0,1,100)\n",
    "\n",
    "y_new = model.params[0] + \\\n",
    "    model.params[1] * x_new + \\\n",
    "    model.params[2] * np.power(x_new,2) + \\\n",
    "    model.params[3] * np.power(x_new,3) + \\\n",
    "    model.params[4] * np.power(x_new,4) + \\\n",
    "    model.params[5] * np.power(x_new,5) + \\\n",
    "    model.params[6] * np.power(x_new,6) + \\\n",
    "    model.params[7] * np.power(x_new,7) + \\\n",
    "    model.params[8] * np.power(x_new,8) + \\\n",
    "    model.params[9] * np.power(x_new,9)\n",
    "\n",
    "plt.scatter(data.x,data.y)\n",
    "plt.plot(x_new,y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions for a 9 term polynomial based approach fits perfectly! This is bad--i.e. for our given data points we've fit a perfect line, but soon as a new datapoint comes in, the prediction is woefully innacurate.\n",
    "\n",
    "* For instance--what about a prediction at x = .9?\n",
    "* Also, what about the correlation of the data for X, X^2, X^3, ..., X^9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All the data is extremely correlated\n",
    "y, X = dmatrices(\"y ~ x + np.power(x,2) + np.power(x,3) + np.power(x,4) + np.power(x,5) + np.power(x,6) + np.power(x,7) + np.power(x,8) + np.power(x,9)\",data=data,return_type=\"dataframe\")\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! The factors are extremely correlated!!\n",
    "\n",
    "This means the beta values/coefficients could change a lot depending on the data that we pass in to train the model.\n",
    "\n",
    "End result? The beta coefficients could have a high variance -> the predictions could have a high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "So far, we've observed a number of cases where we can run into issues with linear regression--all related to overfitting.\n",
    "\n",
    "* We want to use too many factors for how many data we have\n",
    "* Some of the categories have many levels. An example of this could be if there were a categorical variable for \"US State\", of which there would be 50 levels for each US state. We still want to code the effects of the states, but there's not enough data per state.\n",
    "* We want to use similar factors as we believe there's some predictive power, but are afraid that they are all multicollinear\n",
    "* We want to use polynomial terms, but are afraid that they will lead to overfitting\n",
    "\n",
    "\n",
    "Let's explore regularization to solve many of these issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
