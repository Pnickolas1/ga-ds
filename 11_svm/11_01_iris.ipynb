{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfamiliar SVMs with a Familiar Dataset\n",
    "\n",
    "To motivate our basic understandings of SVMs, lets explore SVMs through our favorite iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = iris.target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty standard procedure so far. We've created a design matrix X, and a vector of responses y.\n",
    "\n",
    "We'll also fit, predict, score, etc. in the same way as other sklearn models. Let's take a look at the SVC arguments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at some of the parameters\n",
    "model = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pretty good, though we kind of expected this\n",
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many support vectors exist for each class\n",
    "model.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Indices of the support vectors\n",
    "model.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Locations of the support vectors\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Decision Boundaries\n",
    "\n",
    "We are going to explore what happens to the **decision boundary** and **support vectors** as we change C, kernel, and kernel parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = iris.data[:,[0,2]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(X,y)\n",
    "print \"# of Support Vectors Per Class:\", model.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a grid of points to predict over\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = model.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "\n",
    "plt.contourf(xx,yy,Z,alpha=0.4)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,alpha=0.8)\n",
    "plt.xlabel(iris.feature_names[0]), plt.ylabel(iris.feature_names[2])\n",
    "plt.title(\"Iris Decision Boundaries for SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Use the \"rbf\" (radial basis function) kernel in creating svm.SVC classifier. This is the gaussian drop off we learned.\n",
    "  1. Modulate the gamma parameter from .001 to 100. Observe what happens to the number of support vectors as well as the shape of the decision boundaries as you increase gamma?\n",
    "  2. How does this relate to our conversation of underfitting/overfitting?\n",
    "2. Using the same \"rbf\" kernel, do not supply an argument for gamma. This automatically sets gamma = 1 / n_features.\n",
    "  1. Now, modulate \"C\" from .1 to 1 to 1000. What happens to the decision boundaries? C is the penalty parameter of the errors. High C tells the SVM to work harder to find a less imperfect boundary.\n",
    "3. If you finish early, explore using the \"poly\" (polynomial degree--requires the \"degree\" parameter), as well as the \"linear\" kernel.\n",
    "\n",
    "There are also other SVM classifiers within sklearn including LinearSVC. LinearSVC is slightly different from SVM with a linear kernel.\n",
    "\n",
    "More information here: http://scikit-learn.org/stable/modules/svm.html#svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search\n",
    "\n",
    "Ah, so we've seen that in the case of \"rbf\" or radial basis kernels, we have to modulate the C parameter of the SVM, which modulates how much errors should be avoided. We also have to modulate gamma, which dictates the width of the gaussian kernels.\n",
    "\n",
    "We're going to use GridSearchCV to search for both parameters. In addition, we're going to finalize our example. Remember that SVM uses distance features but we have not normalized any of our features!! We'll start doing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "X[0:5,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X[0:5,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's confirm...\n",
    "print np.mean(X,axis=0)\n",
    "print np.var(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to create a space of C and gamma values to grid search over\n",
    "# And then map it to a dictionary\n",
    "C_range = np.logspace(-3, 10, 13)\n",
    "gamma_range = np.logspace(-13, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2)\n",
    "grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Best Params:\", grid.best_params_\n",
    "print \"Best Score:\", grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how the score changes over range of C and gamma\n",
    "scores = [x[1] for x in grid.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(C_range), len(gamma_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(scores,xticklabels=C_range,yticklabels=gamma_range,cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
