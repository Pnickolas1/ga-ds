{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "<img src=\"http://blog-assets.bigfishgames.com/uploads/2012/08/Choose-your-own-adventure.jpg\"/>\n",
    "\n",
    "Great, now that we've learned the basics of Naive Bayes, it's up to you to choose what path to take. Once you pull the repo, you should have a file called \"07_insult.csv\". This is a csv from a [Kaggle Insult Competition](https://www.kaggle.com/c/detecting-insults-in-social-commentary), where each row contains a comment and has been classified as an Insult or Not.\n",
    "\n",
    "\n",
    "**Using this data, you have the following choices:**\n",
    "\n",
    "* Implement your own insult/not insult classifier using Naive Bayes. Think of this as the Spam example, but instead of Spam/Ham, it is now Insult/NotInsult. (Of course, this generalizes to more than just two classes). It may be easier to calculating probabilties by creatiing two \"documents\" (i.e. word array) of all the insults and all the noninsults. Building your own version will help you get an appreciation for what's going on under the hood of the sklearn implementations. (You can also use the Pandas features for aggregation like I did).\n",
    "\n",
    "    Rather than multiply probabilities for $\\prod{P(w|c)}P(c)$, take the log, and compute $\\sum{log(P(w|c)} +log(P(c))$ i.e. log(x^a * y^b * z^c) == log(x^a) + log(y^b) + log(z^c) == a*log(x) + b*log(y) + c*log(z)\n",
    "\n",
    "    Remember to add Laplace Smoothing to instances of $P(w|c)$\n",
    "\n",
    "\n",
    "* Learn and use SKLearn's MultinomialNB() implementation of Naive Bayes. Similarly, you can use BernoulliNB() as well. The multinomial model (which we learned in class), accounts for multiple occurrences of the same word. The Bernoulli model only counts documents with the presence of the word. Unlike Multinomial models, the Bernoulli model also penalizes for the absence of a word. Multinomial models are generally better when you have many features. Bernoulli models are generally better with fewer, more 'predictive' features, but suffer when these features are noisy.\n",
    "\n",
    "    Count Vectorizer may prove helpful here. (from sklearn.feature_extraction import text; count_vectorizer = text.CountVectorizer() ). Count Vectorizer transforms a corpus (set of documents) into a matrix of token counts, where token counts are the frequency of words.\n",
    "    \n",
    "\n",
    "* If you have used SKLearn or rolled your own Naive Bayes model, what words proved to be most related to an \"insult\"? i.e. If you were to choose ten words to insult someone, what ten words should they be? (Hint: Think about word frequencies)\n",
    "    \n",
    "\n",
    "* All of the above\n",
    "\n",
    "\n",
    "**Possible improvements once you have 1 or 2:**\n",
    "* You may notice from looking at the term frequency matrix (#documents x #terms), that some of these terms seem useless. Perhaps you can preprocess the matrix by dropping columns that have less than x total counts across all documents.\n",
    "* We've created a set of 1-gram features. i.e. each word itself is a feature. But in reality, we might think that sets of words are more useful. Rather than looking at \"not\" and \"terrible\" by themselves, having a feature of \"not terrible\" is much more useful. This would be a 2-gram or bi-gram. This concept extends to n-grams which is a sequence of words of length n. When creating a CountVectorizer, use the option for n_gram_range=(1,xx). This creates a set of features for all of 1-grams (single words), 2-grams (two word sequences), up to xx-grams (xx word sequences). **Don't increase this too high!  n-grams for higher n require a lot more space and computation!**\n",
    "* Naturally, there are a lot of words that don't really contribute to any meaning. \"of\",\"and\",\"it\", etc. These are called stop words. We can remove these through the stop_words=\"english\" option when creating a CountVectorizer.\n",
    "* Similarly, words have a lot of variations. \"Talks\", \"Talked\", \"Talk\" are all essentially the same word, but would be considered different by the tokenizer. Stemming helps remove redundancies by stemming, or truncating words to base words. Try using the [NLTK Stemmer](http://www.nltk.org/howto/stem.html)\n",
    "* Lastly, instead of using word frequencies in the MultinomialNB, try using tf-idf weights instead. [TF-IDF Weighting in SKLearn](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting). tf-idf (term frequency - inverse document frequency) is similar to word counts, but also penalizes when the word shows up in multiple documents.\n",
    "\n",
    "\n",
    "\n",
    "** Additional Helpful Links **\n",
    "\n",
    "[Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "[High Level Overview of Multinomial Naive Bayes](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)\n",
    "\n",
    "[Multinomial vs Bernoulli Naive Bayes](http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/)\n",
    "\n",
    "[Improving Naive Bayes](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('07_insult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult                                            Comment\n",
       "0       1                               \"You fuck your dad.\"\n",
       "1       0  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0  \"listen if you dont wanna get married to a man...\n",
       "4       0  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample SKLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('07_insult.csv') #Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text #Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = text.CountVectorizer() #Create a CountVectorizer object to help convert the documents into a term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer.fit(data.Comment) #Fit the initial data. i.e. Learn all the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_csr = count_vectorizer.transform(data.Comment) #The resulting matrix \"train\" is stored as a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names() # Get the names of the n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dense = X_csr.toarray() # CountVectorizer uses a sparse matrix, but you can get the full dense array as well\n",
    "\n",
    "# If we wanted the column names as well as in a pandas dataframe\n",
    "X_df = pd.DataFrame( X_csr.toarray(),columns=count_vectorizer.get_feature_names() ) \n",
    "y = data.Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB() #Default alpha=1.0. This is equivalent to using the laplace smoothing we mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.fit(X_csr,y) # fitting. i.e. Calculate P(c) for all c. Calculate P(w|c) for all w and c.\n",
    "#nb.fit(X_dense,y) # Same as above\n",
    "#nb.fit(X_df,y)    # Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.score(X_csr,y) # Great! Except we fit and tested on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use cross validation to get a more accurate measure of generalization error\n",
    "# If you pass in X_df (Pandas dataframe), you need to convert it to a \n",
    "# Seems to be a bug where you can't just pass in a Pandas Dataframe for X\n",
    "# Need to convert to a numpy array\n",
    "cvScore = cross_val_score(nb,X_csr,y,cv=10) \n",
    "cvScore = cross_val_score(nb,X_df.values,y,cv=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvScore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
