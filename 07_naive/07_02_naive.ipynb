{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Example\n",
    "\n",
    "We are going to explore using **classification of emails into spam and ham (not spam) using Naive Bayes.**\n",
    "\n",
    "Agenda:\n",
    "* Read in Files\n",
    "* Tokenize with CountVectorize\n",
    "* Examine the data\n",
    "* Generate a few relevant variables in manually calculating Naive Bayes\n",
    "* Use SKLearn's Implementation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate this example, we'll be using a modified form of the [Ling-Spam](http://csmining.org/index.php/ling-spam-datasets.html) dataset. More precisely, this is a set of 960 emails that people have manually read and labeled them as spam or ham.\n",
    "\n",
    "This modified dataset from the Stanford OpenClassroom (and should be available in your repo), performs the following preprocessing steps on the original emails:\n",
    "* Stop Word Removal - remove common, nonmeaningful words such as \"and\",\"the\",\"of\"\n",
    "* Lemmatization - shorten similar words. \"Includes\", \"Included\",\" Include\" are all shortened to \"Include\"\n",
    "* Non-Word Removal - remove numbers, punctuation, tabs. (In practice, perhaps you would want punctuations! i.e. If you see \"Buy this now !!!!!!\", multiple exclamation marks are pretty useful in classifying spam/ham)\n",
    "\n",
    "Finally, I've condensed the entire set of files into a single CSV containing the following columns:\n",
    "* Set: Train/Test - I've explicitly set the emails into train or test sets\n",
    "* Label: Spam/Ham\n",
    "* Text: Words in the email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_repo=\"/Users/brianchung/Desktop/ga-ds/\"\n",
    "data = pd.read_csv(path_to_repo + '/07_naive/07_emailspam.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how the training/testing sample are set up\n",
    "# Technically, cheating by looking at the \"test\" samples as well.\n",
    "# In general, want to discipline ourselves to not looking at test set until the end\n",
    "data.groupby(['Set','Label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's go ahead and for now, split up our data into a train_data, and test_data\n",
    "train_data = data[ data.Set == 'Train']\n",
    "test_data  = data[ data.Set == 'Test' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "To get probabilities for $P(w|c)$, we would need to know how often each word is repeated across the various emails. We COULD go in, split the text, and create a dict of {word : #count}, but its easier to let sklearn do it for us.\n",
    "\n",
    "\n",
    "## CountVectorizer\n",
    "\n",
    "`sklearn` comes with many built-in feature extraction and manipulation tools. For dealing with text data, there is the  `sklearn.feature_extraction.text` module, which contains the **`CountVectorizer`**\n",
    "\n",
    "CountVectorizer is a class that transforms text (either in the form of a list of strings, dataframe columns, Series), and outputs a matrix of document x tokens (where tokens represent a word or a phrase).\n",
    "\n",
    "For instance, if we had a 2 element array of [\"apple is an apple\", \"why is this blue\"], the output matrix would look like this:\n",
    "\n",
    "is|an|apple|why|this|blue\n",
    "--|--|-----|---|----|----\n",
    " 1| 1|    2|  0|   0|   0\n",
    " 1| 0|    0|  1|   1|   1\n",
    "\n",
    ",where the first row represents the tokenization of the first sentence \"apple is an apple\"\n",
    ", and the second row represents the tokenziation of the second sentence \"why is this blue\".\n",
    "\n",
    "\n",
    "The `CountVectorizer` (and most feature extraction methods in sklearn) follows a very simple interface:\n",
    "- `fit` takes a dataset and learns the features it's trying to extract. In this case that means that the algorithm learns the vocabulary of all samples\n",
    "- `transform` takes a dataset and produces the matrix as described above, based on the vocabulary (or feature elements) it learned.\n",
    "- `fit_transform` combines the two steps at once.\n",
    "\n",
    "For example, you may want to fit a vocabulary to a training set, transform the training set to train a model and then continually transform any new incoming examples you want to classify. You will generally only perform the fit step once but the transform step many times for any new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit learns the vocabularies within the training data\n",
    "vect.fit(train_data.Text)\n",
    "\n",
    "# When we pass in a set of documents to transform, countvectorizer returns a matrix of frequencies\n",
    "# with only words that have been fitted into the vocabulary\n",
    "train_csr = vect.transform(train_data.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It returns a sparse matrix, which is more efficient when only a few entries of the matrix are non-zero\n",
    "print type(train_csr), '\\n'\n",
    "print train_csr.shape, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can also convert to a dense (full) matrix\n",
    "print train_csr.toarray(),'\\n'\n",
    "\n",
    "print train_csr.toarray().shape,'\\n'\n",
    "print train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well, it has the same number of documents, but what are the words?\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can convert to a pandas dataframe too (generally not needed for SKLearn methods)\n",
    "train_df = pd.DataFrame( train_csr.toarray(), columns=vect.get_feature_names() )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many times does 'aa' occur throughout all the training documents?\n",
    "print train_df.columns[0]\n",
    "print train_df.iloc[:,0].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many tokens are there per row? \n",
    "# I.E. How many words are there per email? \n",
    "# Tokens are individual words in this case \n",
    "train_df.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Whats the most frequent token?\n",
    "token_count = train_df.sum(axis=0)\n",
    "\n",
    "print \"Max:\",  np.max(token_count)\n",
    "print \"Word:\", np.argmax(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the total word frequencies for spam emails?\n",
    "train_labels = train_data.Label\n",
    "train_df[ train_labels == 'Spam'].sum(axis=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "If you have not yet already, review and run the code blocks up to this point.\n",
    "\n",
    "You will need the following defined (at least):\n",
    "* train_data\n",
    "* train_labels\n",
    "* train_df \n",
    "\n",
    "**Count the total number of emails in train_data. Save this to \"doc_total\"**\n",
    "\n",
    "**Count the total number of spam emails in train_data. Save this to \"doc_spam\"**\n",
    "\n",
    "**Count the total number of ham emails in train_data. Save this to \"doc_ham\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert(doc_spam + doc_ham == doc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many total unique words are there in the training set (Vocabulary)? Hint: This is the number of columns in train_df. Save this to \"V_train\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "assert(V_train==19073)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the total count of words in spam emails? Save this as \"word_total_spam\". **\n",
    "\n",
    "**What is the total count of words in ham emails? Save this as \"word_total_ham\". **\n",
    "\n",
    "i.e. if there are two spam emails:\n",
    "\n",
    "    A: \"hello world hello\"\n",
    "    B: \"is spam\"\n",
    "then word_total_spam = 3 + 2 = 5\n",
    "\n",
    "Hint: Hint: Subset train_df on the labels, and use 'sum' to get total frequencies similar. You may need to call xx.sum().sum() to get the sum of every element where xx is the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "\n",
    "assert(word_total_spam==105771 and word_total_ham==86102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the total counts per word per class.**\n",
    "\n",
    "Hint: Subset train_df on the labels, and use 'sum' to get total frequencies similar to above (Use the axis= argument as well in sum).\n",
    "\n",
    "Save the resulting series as **word_count_ham**, and **word_count_spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "assert( len(word_count_spam)==19073 and len(word_count_ham)==19073 and \n",
    "       np.sum(word_count_spam)==105771 and np.sum(word_count_ham)==86102 and\n",
    "      type(word_count_spam) is pd.Series and type(word_count_ham) is pd.Series )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is the probability of seeing the word \"web\" given class == \"Spam\" ? What is the probability of seeing the word \"web\" given class == \"Ham\" ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What is the probability of seeing the word \"Brrahhhhh\" given class == \"Spam\"? What about \"Ham\"?**\n",
    "\n",
    "Hint: Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now you're ready to classify documents! Why? Let's think about it\n",
    "\n",
    "* You can calculate class priors P(C==ham) and P(C==spam). This is just doc_ham/doc_total, and doc_spam/doc_total.\n",
    "* You can also iterate through the class labels, and generate P(w|C==ham) and p(w|C==spam). This is just the count of the word `w` divided by the total number of words in the class.\n",
    "* In essence, if there were a `NaiveBayes` model in SKLearn, it's as if you had calculate all the components of `fit`\n",
    "\n",
    "```\n",
    "Algorithm:\n",
    "    Calculate class Priors p(spam), p(ham)\n",
    "    For each row in the data:\n",
    "        Keep a running total probability for spam and ham (logP_spam, logP_ham)\n",
    "        For each word in row:\n",
    "            Calculate P(w|spam), then add this to the running total probability for spam\n",
    "            [P(w|spam)= data frequency of w * log( (word_count_spam[w] + 1) / (word_total_spam + V_train))] \n",
    "            Calculate P(w|ham), then add this to the running total probability for ham\n",
    "            [P(w|ham) = data frequency of w *  log((word_count_ham[w] + 1) / (word_total_ham + V_train))] \n",
    "        Compare logP_spam and logP_ham. Classify this row as whichever is higher\n",
    "            \n",
    "```\n",
    "\n",
    "\n",
    "I've included my version of Naive Bayes using a Multinomial model, but you will have the opportunity to write your own later as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Prediction on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recreate a count vectorizer here, and fit it on the training data\n",
    "vect = CountVectorizer()\n",
    "vect.fit(train_data.Text)\n",
    "\n",
    "# Transform the training data into a sparse matrix, and then to a pandas dataframe for ease\n",
    "train_csr = vect.transform(train_data.Text)\n",
    "train_df = pd.DataFrame( train_csr.toarray(), columns=vect.get_feature_names() )\n",
    "\n",
    "\n",
    "# Save a list of the truth and one for my predictions\n",
    "all_true_labels = train_data.Label\n",
    "all_pred_labels = [\"\"]*len(train_data.Label)\n",
    "\n",
    "\n",
    "# Calculate our class priors\n",
    "p_c_spam = np.float(doc_spam) / np.float(doc_total)\n",
    "p_c_ham  = np.float(doc_ham) / np.float(doc_total)\n",
    "\n",
    "for row_idx in range(train_data.shape[0]):\n",
    "\n",
    "    \n",
    "    # Save a running log probability \n",
    "    logP_spam = np.float64(0.0)\n",
    "    logP_ham  = np.float64(0.0)\n",
    "    \n",
    "    # Grab all the tokens for that row in the form of a Series w/index = word, value = frequency of word\n",
    "    words = train_df.iloc[row_idx,:]\n",
    "    words = words[ words > 0]\n",
    "    \n",
    "    for word in words.index:\n",
    "        cnt = words[word]\n",
    "\n",
    "        #Calculate the quantity p(w|spam)\n",
    "        p_word_spam = 0.0\n",
    "        if word in word_count_spam.index:\n",
    "            p_word_spam = (word_count_spam[word] + 1.0) / ( np.float(word_total_spam) + V_train )\n",
    "        else:\n",
    "            # If this word is not in our dictionary, give it the laplace smoothed prior!\n",
    "            p_word_spam = 1.0 / ( np.float(word_total_spam) + V_train )\n",
    "\n",
    "\n",
    "        #Calculate the quantity p(w|spam)\n",
    "        p_word_ham = 0.0\n",
    "        if word in word_count_ham.index:\n",
    "            p_word_ham = (word_count_ham[word] + 1.0) / ( np.float(word_total_ham) + V_train )\n",
    "        else:\n",
    "            # If this word is not in our dictionary, give it the laplace smoothed prior!\n",
    "            p_word_ham = 1.0 / ( np.float(word_total_ham) + V_train )\n",
    "        \n",
    "        # Add the correct count * log( p(w|c) ) to both categories\n",
    "        logP_spam = logP_spam + cnt * np.log(p_word_spam)\n",
    "        logP_ham = logP_ham +   cnt * np.log(p_word_ham)\n",
    "\n",
    "    logP_spam = logP_spam + np.log(p_c_spam)\n",
    "    logP_ham = logP_ham + np.log(p_c_ham)\n",
    "\n",
    "    # Check which value is higher. Remember we didn't compute any normalization, since we don't care\n",
    "    if logP_spam > logP_ham: \n",
    "        all_pred_labels[row_idx] = \"Spam\"\n",
    "    else:\n",
    "        all_pred_labels[row_idx] = \"Ham\"\n",
    "\n",
    "\n",
    "print \"Accuracy:\", np.sum(all_pred_labels == all_true_labels) / np.float(len(all_true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Prediction on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test set Using the Probabilities we've calculated with the training set\n",
    "# !!Notice that for p(w|c) we are still using the probabilities off the training set!\n",
    "\n",
    "\n",
    "# Similar to before, but we don't call fit again! We use the same vocabulary as in the training set\n",
    "test_csr = vect.transform(test_data.Text)\n",
    "test_df = pd.DataFrame( test_csr.toarray(), columns=vect.get_feature_names() )\n",
    "\n",
    "\n",
    "all_true_labels = test_data.Label\n",
    "all_pred_labels = [\"\"]*len(test_data.Label)\n",
    "\n",
    "p_c_spam = np.float(doc_spam) / np.float(doc_total)\n",
    "p_c_ham  = np.float(doc_ham) / np.float(doc_total)\n",
    "\n",
    "for row_idx in range(test_data.shape[0]):\n",
    "    \n",
    "    # Save a running log probability \n",
    "    logP_spam = np.float(0.0)\n",
    "    logP_ham  = np.float(0.0)\n",
    "    \n",
    "    words = test_df.iloc[row_idx,:]\n",
    "    words = words[ words > 0 ]\n",
    "\n",
    "    for word in words.index:\n",
    "        cnt = np.float(words[word])\n",
    "        \n",
    "        p_word_spam = 0.0\n",
    "        if word in word_count_spam.index:\n",
    "            p_word_spam = (word_count_spam[word] + 1.0) / ( np.float(word_total_spam) + V_train )\n",
    "        else:\n",
    "            # Give it the laplace smoothed prior!\n",
    "            p_word_spam = 1.0 / ( np.float(word_total_spam) + V_train )\n",
    "\n",
    "        p_word_ham = 0.0\n",
    "        if word in word_count_ham.index:\n",
    "            p_word_ham = (word_count_ham[word] + 1.0) / ( np.float(word_total_ham) + V_train )\n",
    "        else:\n",
    "            # Give it the laplace smoothed prior!\n",
    "            p_word_ham = 1.0 / ( np.float(word_total_ham) + V_train )\n",
    "        \n",
    "        logP_spam = logP_spam + cnt * np.log(p_word_spam)\n",
    "        logP_ham  = logP_ham  + cnt * np.log(p_word_ham)\n",
    "        \n",
    "    logP_spam = logP_spam + np.log(p_c_spam)\n",
    "    logP_ham = logP_ham + np.log(p_c_ham)\n",
    "    \n",
    "    if logP_spam > logP_ham: \n",
    "        all_pred_labels[row_idx] = \"Spam\"\n",
    "    else:\n",
    "        all_pred_labels[row_idx] = \"Ham\"\n",
    "\n",
    "\n",
    "print \"Accuracy:\", np.sum(all_pred_labels == all_true_labels)/ np.float(len(all_true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quite a lot of work (and probably poor coding) to create a multinomial Naive Bayes solution. You'll have a chance later on the improve upon this and write your own version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn `naive_bayes`\n",
    "\n",
    "For now, let's also see how SKLearn's implementation of naive bayes operates.\n",
    "\n",
    "There are many variants of Naive Bayes in the module, and they all mostly differ in how they encode P(x|c) (or the sampling model)\n",
    "* MultinomialNB() : What we've learned so far, where multiple counts of words increase the probability of a class\n",
    "* BernoulliNB(): Where the presence or absence (1 and 0s, not counts) will increase or decrease the probability of a class\n",
    "* GaussianNB(): Where the probability of a class is based on how far this feature is from the center of the class. This would be good for continuous normally distributed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(train_data.Text)\n",
    "\n",
    "mm = MultinomialNB()\n",
    "\n",
    "\n",
    "train_csr = vect.transform(train_data.Text)\n",
    "y_train = train_data.Label\n",
    "\n",
    "mm.fit(train_csr,y_train)\n",
    "print \"Training:\", mm.score(train_csr,y_train)\n",
    "\n",
    "test_csr = vect.transform(test_data.Text)\n",
    "y_test = test_data.Label\n",
    "print \"Testing:\", mm.score(test_csr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(train_data.Text)\n",
    "\n",
    "mm = BernoulliNB()\n",
    "\n",
    "\n",
    "train_csr = vect.transform(train_data.Text)\n",
    "y_train = train_data.Label\n",
    "\n",
    "mm.fit(train_csr,y_train)\n",
    "print \"Training:\", mm.score(train_csr,y_train)\n",
    "\n",
    "test_csr = vect.transform(test_data.Text)\n",
    "y_test = test_data.Label\n",
    "print \"Testing:\", mm.score(test_csr,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams\n",
    "\n",
    "With CountVectorizer, we don't need to only have single words as tokens. We can have phrases of any length as tokens.\n",
    "\n",
    "For instance, with the default way, the sentence \"DONT BUY\" would be tokenized into \"DONT\", \"BUY\". However, we would think the total phrase of length 2 (aka 2-gram, or bi-gram), is important.\n",
    "\n",
    "Using the ngram_range feature in CountVectorizer, we can specify the range of n-grams we want. For instance, ngram_range=(1,5), would generate n-grams of length 1 (single words), length 2 (2 word phrases), length 3 (3 word phrases), and so on to 5.\n",
    "\n",
    "Be careful! Setting ngram_range too high is not likely to help, and it increases the computing cost greatly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Similar to above, but I've used the ngram_range argument in creating the CountVectorizer\n",
    "ngram2vect = CountVectorizer(ngram_range=(1,2))\n",
    "ngram2vect.fit(train_data.Text)\n",
    "ngram2_fn = ngram2vect.get_feature_names()\n",
    "ngram2_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helpful tools in diagnosing your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(train_data.Text)\n",
    "\n",
    "train_csr = vect.transform(train_data.Text)\n",
    "y_train = train_data.Label\n",
    "\n",
    "mm = MultinomialNB()\n",
    "mm.fit(train_csr,y_train)\n",
    "\n",
    "print mm.score(train_csr,y_train)\n",
    "print mm.class_count_\n",
    "print mm.class_log_prior_\n",
    "print mm.class_prior\n",
    "print mm.classes_\n",
    "print mm.predict_log_proba(train_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(MultinomialNB(), train_csr, y_train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
